# ğŸ§° Jobinja Web Scraper ğŸ§‘â€ğŸ’»ğŸ”

## Overview

This project is a ğŸ Python-based web scraper designed to systematically extract ğŸ“ job listings from the Jobinja ğŸŒ website. The scraper ğŸ—ƒï¸ collects, ğŸ”„ preprocesses, and ğŸ§½ cleanses data, saves it in multiple ğŸ“ formats, and performs elementary ğŸ“Š descriptive statistics. The following sections provide an in-depth ğŸ“‹ description of the core components of the class, along with illustrative examples of the results.

The purpose of this scraper is to facilitate efficient data extraction from the Jobinja ğŸŒ website, thereby automating the traditionally labor-intensive â›ï¸ task of gathering job-related ğŸ“ data. The scraper not only extracts raw information but also transforms it into structured, machine-readable formats ğŸ–¥ï¸ for further analysis. By enabling customization of output formats, users can derive insights specific to their needs, supporting data-driven ğŸ“ˆ decision-making in the employment domain. Ultimately, this project aims to automate data collection and offer robust access to high-quality ğŸ” data that supports the analytical process.

The web scraperâ€™s modularity enables it to adapt to different sections of the ğŸŒ website, allowing for a targeted ğŸ¯ approach based on job categories, employment types, or other user-specific criteria. By leveraging automation, this tool alleviates the repetitive ğŸ” nature of manual job searching. It can aggregate information on job titles, companies, locations, required qualifications, and additional metadata, thereby empowering users to make well-informed career ğŸ’¼ decisions. By structuring the data efficiently, users can conduct trend analysis, compare opportunities, and even create predictive models ğŸ“Š for emerging job market trends.

The data extracted by the scraper can further be used to build analytical dashboards ğŸ“Š, generate comprehensive reports ğŸ“‘, or be utilized in machine learning ğŸ¤– models to identify optimal employment opportunities. This makes the scraper a valuable tool for data scientists ğŸ§ , job market analysts ğŸ“Š, and individuals interested in automating their job search processes.

## ğŸ“¦ Class Overview

The `Jobinja` class offers functionalities to scrape, preprocess, and manage data extracted from the Jobinja platform ğŸŒ. The class design ensures effective and efficient extraction of all key components associated with job listings. Additionally, the modular structure allows for extension and adaptation, facilitating scraping of other similar platforms or the integration of new features.

### `Jobinja` Class Methods

The `Jobinja` class comprises several key methods, each of which plays a pivotal role in the data extraction pipeline:

- **`__init__()`**: ğŸ—ï¸ Initializes the scraper, sets HTTP headers ğŸ“¨ to simulate browser requests, creates necessary directories ğŸ“‚ for data storage, and configures logging ğŸ“œ for capturing errors. This setup guarantees that all prerequisites are in place for seamless scraper operation.
  
- **`get_links()`**: Gathers unique ğŸ”— hyperlinks from the main page, which serve as entry points for crawling ğŸ•·ï¸ job listings. This method sends an initial GET request to the Jobinja website ğŸŒ and parses the HTML to identify subpages containing job information. The use of a queue ğŸ“¥ facilitates efficient link management, supporting breadth-first crawling.

- **`clean_persian_text(text)`**: Cleanses Persian ğŸ“ text by removing extraneous whitespace and non-Persian characters. This preprocessing step ensures data quality, consistency, and standardization, which are vital for subsequent analytical procedures.

- **`extract_job_features(subpage_soup)`**: Extracts job-specific features such as job title ğŸ“›, description ğŸ“„, category ğŸ“‚, location ğŸ“, employment type ğŸ‘¨â€ğŸ’¼, and more. This method identifies relevant HTML elements where job information resides and processes the text to generate a structured output. This approach ensures that all relevant attributes are accurately captured and standardized.

- **`scrape_jobs()`**: Iterates ğŸ”„ over collected hyperlinks to visit each job listing, invokes the `extract_job_features` method to gather relevant data, and subsequently saves the cleaned data into text files ğŸ“ for further processing or analysis. This method also incorporates error handling ğŸ› ï¸ to log any issues that arise during requests and retries.

- **`save_dataset()`**: Saves the collected dataset into both `.mat` and `.json` formats ğŸ“. The `.mat` format is tailored for MATLAB integration, while `.json` offers a versatile format suitable for web-based applications and advanced analytics.

- **`display_dataset()`**: Converts the dataset into a pandas DataFrame ğŸ“Š, making it easy to visualize ğŸ‘€, explore, and manipulate the data in a tabular format. This method is highly beneficial for conducting preliminary data analysis.

- **`descriptive_statistics()`**: Generates descriptive ğŸ“ˆ statistics for the dataset, providing an overview of features such as job title frequencies, distribution across job categories, and other key attributes. These statistics offer valuable initial insights that inform subsequent analysis.

### Method Details

1. **Initialization (`__init__`)**:
   - Configures HTTP headers to prevent request blocking ğŸš« by mimicking a legitimate user agent.
   - Creates a designated save directory ğŸ—‚ï¸ for data files.
   - Sets up a rotating file handler for logging ğŸ”„ to ensure that log files do not grow indefinitely and overwhelm the system.

2. **Collecting Links (`get_links`)**:
   - Sends an HTTP GET request to the main page of the Jobinja platform ğŸŒ.
   - Parses the HTML using BeautifulSoup ğŸ² to locate all anchor (`<a>`) tags.
   - Uses a queue ğŸ“¥ to maintain the order of links for breadth-first crawling, ensuring systematic coverage of the job listings.

3. **Cleaning Text (`clean_persian_text`)**:
   - Employs regular expressions ğŸ” to remove non-Persian characters and extraneous whitespace.
   - Ensures that text data is consistent across the dataset, which is crucial for effective analysis and machine learning modeling ğŸ¤–.

4. **Extracting Job Features (`extract_job_features`)**:
   - Extracts features such as job title ğŸ“›, job description ğŸ“„, category ğŸ“‚, and location ğŸ“ from a subpage.
   - Stores the cleaned information in a structured dictionary ğŸ“‘.
   - Handles missing fields by assigning default values to maintain dataset integrity and avoid potential issues during analysis.

5. **Scraping Jobs (`scrape_jobs`)**:
   - Iterates through each subpage link collected via `get_links()` ğŸ”—.
   - Issues GET requests to each link and invokes `extract_job_features()` to collect and process relevant data.
   - Implements a delay â±ï¸ between requests to prevent overwhelming the server and risking IP blocks.
   - Saves data in `.txt` files ğŸ“„, providing a straightforward and accessible format for users.

## âš™ï¸ Installation and Usage

To deploy the `Jobinja` scraper, follow these steps:

1. **Clone the Repository** ğŸ™
   ```bash
   git clone <repository-url>
   cd jobinja_scraper
   ```

2. **Install Dependencies** ğŸ“¦
   Ensure you have the necessary Python libraries installed:
   ```bash
   pip install requests beautifulsoup4 scipy pandas
   ```

3. **Run the Script** ğŸš€
   To scrape job listings and save the dataset (saved in the `JobInja` directory ğŸ“):
   ```bash
   python jobinja_scraper.py
   ```

4. **Customize the Scraper**:
   - Modify the `base_url` ğŸŒ to target different parts of the Jobinja website as required.
   - Adjust logging levels ğŸ“œ to capture more or less detail, depending on debugging needs.
   - Change the delay â±ï¸ in the `scrape_jobs()` method to accommodate different server request limits.
   - Update `extract_job_features()` to capture additional fields or features relevant to the analysis.

## ğŸ“ Example Output

### Extracted Job Features

An example of the output from a scraped job listing is shown below:

```
ğŸ”— URL: https://jobinja.ir/companies/world-ocean-oil-company/jobs/A4VV/...
ğŸ“ Job_title: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø§Ù†Ù…
ğŸ“„ Job_description: Ù‡Ù…ÛŒÙ† Ø­Ø§Ù„Ø§ Ø±Ø²ÙˆÙ…Ù‡ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± Ú©Ù…ØªØ± Ø§Ø² Û±Û° Ø¯Ù‚ÛŒÙ‚Ù‡ Ø¨Ø³Ø§Ø²ÛŒØ¯...
ğŸ“‚ Job_category: ÙØ±ÙˆØ´ Ùˆ Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒ
ğŸ“ Job_location: ØªÙ‡Ø±Ø§Ù†
ğŸ‘¨â€ğŸ’¼ Employment_type: ØªÙ…Ø§Ù…â€ŒÙˆÙ‚Øª
ğŸ“† Min_experience: 1 Ø³Ø§Ù„
ğŸ’° Salary: ØªÙˆØ§ÙÙ‚ÛŒ
ğŸš» Gender: Ø®Ø§Ù†Ù…
ğŸ›¡ï¸ Military_status: Ù…Ø¹Ø§ÙÛŒØª Ø¯Ø§Ø¦Ù…
ğŸ“ Education_level: Ú©Ø§Ø±Ø´Ù†Ø§Ø³ÛŒ
ğŸ¢ Company_intro: Ø´Ø±Ú©Øª Ø§Ù‚ÛŒØ§Ù†ÙˆØ³ Ø¬Ù‡Ø§Ù†ÛŒ Ù†ÙØª ÙØ¹Ø§Ù„ÛŒØª Ø®ÙˆØ¯ Ø±Ø§...
ğŸ› ï¸ Skills_required: ØªØ³Ù„Ø· Ø¨Ù‡ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø±Ù‡Ø§ÛŒ Ù…Ø§ÛŒÚ©Ø±ÙˆØ³Ø§ÙØª Ø¢ÙÛŒØ³
ğŸ” Content_snippet: Ù‡Ù…ÛŒÙ† Ø­Ø§Ù„Ø§ Ø±Ø²ÙˆÙ…Ù‡ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± Ú©Ù…ØªØ± Ø§Ø² Û±Û° Ø¯Ù‚ÛŒÙ‚Ù‡ Ø¨Ø³Ø§Ø²ÛŒØ¯...
```

### Descriptive Statistics

Upon scraping job listings, the script can generate descriptive ğŸ“ˆ statistics for the dataset:

```
          job_title job_category job_location employment_type ...
count           50            50           50             50 ...
unique          30             8            6              3 ...
top     Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø§Ù†Ù…     ÙØ±ÙˆØ´ Ùˆ Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒ        ØªÙ‡Ø±Ø§Ù†      ØªÙ…Ø§Ù…â€ŒÙˆÙ‚Øª ...
freq            10            20           18             25 ...
```

These descriptive statistics provide an overall view of the dataset, allowing users to understand the diversity of job listings and their respective distributions. By highlighting key trendsâ€”such as in-demand job categories or common employment typesâ€”the analysis can yield valuable insights for both job seekers and researchers.

## ğŸš€ Features to Improve

- **Adaptive Rate-Limiting**: Currently, a fixed delay of 1 second â±ï¸ is added between requests to avoid overwhelming the server. This could be enhanced by implementing adaptive rate limiting that dynamically adjusts based on server response times, thus improving efficiency âš¡.
- **Error Logging Enhancements**: The logging mechanism ğŸ“œ employs a rotating file handler ğŸ”„ to prevent log files from growing unchecked. More granular and detailed error messages would further facilitate debugging and improve the scraper's robustness.
- **Retry Mechanism**: Introducing a retry mechanism ğŸ” for failed HTTP requests would significantly enhance the reliability of the scraper, particularly when scraping a large number of job listings, where transient network failures are common. This would improve the scraper's fault tolerance, ensuring more comprehensive data collection ğŸ“Š.

## ğŸ“œ License

This project is licensed under the MIT License âš–ï¸, ensuring it is freely available for use, modification, and distribution. Contributions are welcomed ğŸ¤, with the aim to foster collaboration and knowledge-sharing among those interested in web scraping, data analysis, and automation ğŸ¤–.

---

We invite contributions ğŸ¤², suggestions for improvements ğŸ’¡, and the use of this scraper for educational ğŸ“ or research purposes. Whether you aim to enhance your web scraping skills, automate job searches ğŸ”, or conduct research on labor market trends, this project offers a robust starting point ğŸš€. We welcome any improvements that can make the scraper more efficient, reliable, and useful. As a community, we look forward to evolving this tool to address emerging challenges and opportunities in data scraping and analysis ğŸ§ .

